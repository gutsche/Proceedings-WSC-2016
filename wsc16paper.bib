@article{1748-0221-3-08-S08001,
  author={Lyndon Evans and Philip Bryant},
  title={LHC Machine},
  journal={Journal of Instrumentation},
  volume={3},
  number={08},
  pages={S08001},
  url={http://stacks.iop.org/1748-0221/3/i=08/a=S08001},
  year={2008},
  abstract={The Large Hadron Collider (LHC) at CERN near Geneva is the world's newest and most powerful tool for Particle Physics research. It is designed to collide proton beams with a centre-of-mass energy of 14 TeV and an unprecedented luminosity of 10 34 cm −2 s −1 . It can also collide heavy (Pb) ions with an energy of 2.8 TeV per nucleon and a peak luminosity of 10 27 cm −2 s −1 . In this paper, the machine design is described.}
}
@proceedings{Goddard:1968515,
      author        = "Goddard, B and Dubourg, S",
      title         = "{5th Evian Workshop on LHC beam operation}",
      organization  = "CERN",
      publisher     = "CERN",
      address       = "Geneva",
      year          = "2014",
      reportNumber  = "CERN-ACC-2014-0319",
      url           = "https://cds.cern.ch/record/1968515",
      note          = "Organisers: Lamont, M; Meddahi, M; Goddard, B",
}
@article{1748-0221-10-02-C02030,
  author={S. Artz et. al.},
  title={Upgrade of the ATLAS Central Trigger for LHC Run-2},
  journal={Journal of Instrumentation},
  volume={10},
  number={02},
  pages={C02030},
  url={http://stacks.iop.org/1748-0221/10/i=02/a=C02030},
  year={2015},
  abstract={The increased energy and luminosity of the LHC in the run-2 data taking period requires a more selective trigger menu in order to satisfy the physics goals of ATLAS. Therefore the electronics of the central trigger system is upgraded to allow for a larger variety and more sophisticated trigger criteria. In addition, the software controlling the central trigger processor (CTP) has been redesigned to allow the CTP to accommodate three freely configurable and separately operating sets of sub detectors, each independently using the almost full functionality of the trigger hardware. This new approach and its operational advantages are discussed as well as the hardware upgrades.}
}
@techreport{Bawej:1711011,
      author        = "Bawej, Tomasz Adrian et. al.",
      title         = "{The New CMS DAQ System for Run 2 of the LHC}",
      institution   = "CERN",
      address       = "Geneva",
      number        = "CMS-CR-2014-082",
      month         = "May",
      year          = "2014",
      reportNumber  = "CMS-CR-2014-082",
      url           = "https://cds.cern.ch/record/1711011",
}
@misc{EGIWikiGlossary,
  title = "{EGI Glossary:}",
  howpublished = {\url{https://wiki.egi.eu/wiki/Glossary_V1}},
}
@article{Bird:2005js,
      author         = "Bird, I. and Bos, K. and Brook, N. and Duellmann, D. and
                        Eck, C. and others",
      editor         = "Bird, I.",
      title          = "{LHC computing Grid. Technical design report}",
      year           = "2005",
      reportNumber   = "CERN-LHCC-2005-024",
      SLACcitation   = "%%CITATION = CERN-LHCC-2005-024 ETC.;%%",
}
@inproceedings{Sfiligoi:2009:PWG:1579192.1579413,
 author = {Sfiligoi, Igor and Bradley, Daniel C. and Holzman, Burt and Mhashilkar, Parag and Padhi, Sanjay and Wurthwein, Frank},
 title = {The Pilot Way to Grid Resources Using glideinWMS},
 booktitle = {Proceedings of the 2009 WRI World Congress on Computer Science and Information Engineering - Volume 02},
 series = {CSIE '09},
 year = {2009},
 isbn = {978-0-7695-3507-4},
 pages = {428--432},
 numpages = {5},
 url = {http://dx.doi.org/10.1109/CSIE.2009.950},
 doi = {10.1109/CSIE.2009.950},
 acmid = {1579413},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {glideinWMS, Condor, Virtual Private Computing Pools, Grid computing},
} 
@article{condor-practice, 
  author    = "Douglas Thain and Todd Tannenbaum and Miron Livny",
  title     = "Distributed computing in practice: the Condor experience.",
  journal   = "Concurrency - Practice and Experience",
  volume    = "17",
  number    = "2-4",
  year      = "2005",
  pages     = "323-356",
}
@article{Blum:2013mhx,
       author         = "Blum, T. and Van de Water, R.S. and Holmgren,
D. and
                         Brower, R. and Catterall, S. and others",
       title          = "{Working Group Report: Lattice Field Theory}",
       year           = "2013",
       eprint         = "1310.6087",
       archivePrefix  = "arXiv",
       primaryClass   = "hep-lat",
       reportNumber   = "FERMILAB-CONF-13-483-T",
       SLACcitation   = "%%CITATION = ARXIV:1310.6087;%%",
}
@article{Amundson:2014rqa,
      author         = "Amundson, James and Macridin, Alexandru and Spentzouris,
                        Panagiotis",
      title          = "{High Performance Computing Modeling Advances Accelerator
                        Science for High Energy Physics}",
      journal        = "IEEE Comput.Sci.Eng.",
      number         = "6",
      volume         = "16",
      pages          = "32-41",
      doi            = "10.1109/MCSE.2014.76",
      year           = "2014",
      reportNumber   = "FERMILAB-PUB-14-117-CD",
      SLACcitation   = "%%CITATION = ISCEE,16,32;%%",
}
@article{Chatrchyan:2012ufa,
      author         = "Chatrchyan, Serguei and others",
      title          = "{Observation of a new boson at a mass of 125 GeV with the
                        CMS experiment at the LHC}",
      collaboration  = "CMS",
      journal        = "Phys.Lett.",
      volume         = "B716",
      pages          = "30-61",
      doi            = "10.1016/j.physletb.2012.08.021",
      year           = "2012",
      eprint         = "1207.7235",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      reportNumber   = "CMS-HIG-12-028, CERN-PH-EP-2012-220",
      SLACcitation   = "%%CITATION = ARXIV:1207.7235;%%",
}
@misc{SDSCPressReleaseCMS,
  title = "{Press Release: SDSC’s Gordon Supercomputer Assists in Crunching Large Hadron Collider Data}",
  howpublished = {\url{http://ucsdnews.ucsd.edu/pressrelease/sdscs_gordon_supercomputer_assists_in_crunching_large_hadron_collider_data}},
}
@Article{alpgen,
     author    = "Mangano, Michelangelo L. and Moretti, Mauro and Piccinini,
                  Fulvio and Pittau, Roberto and Polosa, Antonio D.",
     title     = "{ALPGEN, a generator for hard multiparton processes in
                  hadronic  collisions}",
     journal   = "JHEP",
     volume    = "07",
     year      = "2003",
     pages     = "001",
     eprint    = "hep-ph/0206293",
     archivePrefix = "arXiv",
     doi       = "10.1088/1126-6708/2003/07/001"
}
@inproceedings{CHEP2015:Mira,
 author = {Childers, T. and Le Compte, T. and Uram, T. and Benjamin, D.},
 title = {Simulation of LHC events on a million threads},
 booktitle = {21st International Conference on Computing in High Energy and Nuclear Physics (CHEP2015)},
 url = {https://indico.cern.ch/event/304944/},
 year = {2015},
}
@article{1742-6596-78-1-012057,
  author={Ruth Pordes and Don Petravick and Bill Kramer and Doug Olson and Miron Livny and Alain Roy and Paul Avery and Kent
Blackburn and Torre Wenaus and Frank Würthwein and Ian Foster and Rob Gardner and Mike Wilde and Alan Blatecky and John
McGee and Rob Quick},
  title={The Open Science Grid},
  journal={Journal of Physics: Conference Series},
  volume={78},
  number={1},
  pages={012057},
  url={http://stacks.iop.org/1742-6596/78/i=1/a=012057},
  year={2007},
  abstract={The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.}
}
@incollection{EGI:2012,
year={2010},
isbn={978-1-4419-5595-1},
booktitle={Remote Instrumentation and Virtual Laboratories},
editor={Davoli, Franco and Meyer, Norbert and Pugliese, Roberto and Zappatore, Sandro},
doi={10.1007/978-1-4419-5597-5_6},
title={The European Grid Initiative (EGI)},
url={http://dx.doi.org/10.1007/978-1-4419-5597-5_6},
publisher={Springer US},
keywords={Grid computing; Grid technologies; e-Infrastructures; Sustainability; European grid infrastructure; National grid initiatives},
author={Kranzlmüller, D. and de Lucas, J.Marco and Öster, P.},
pages={61-66},
language={English}
}
@inproceedings{CHEP2015:FermiCloud,
 author = {Timm, S. and Garzoglio, G. and others},
 title = {Cloud services for the Fermilab scientific stakeholders},
 booktitle = {21st International Conference on Computing in High Energy and Nuclear Physics (CHEP2015)},
 url = {https://indico.cern.ch/event/304944/session/7/contribution/448},
 year = {2015},
}
@inproceedings{CHEP2015:CloudAtlas,
 author = {Taylor, R and others},
 title = {Evolution of Cloud Computing in ATLAS},
 booktitle = {21st International Conference on Computing in High Energy and Nuclear Physics (CHEP2015)},
 url = {https://indico.cern.ch/event/304944/session/7/contribution/146},
 year = {2015},
}
@article{1742-6596-368-1-012011,
  author={Jan Balewski and Jerome Lauret and Doug Olson and Iwona Sakrejda and Dmitry Arkhipkin and John Bresnahan and Kate
Keahey and Jeff Porter and Justin Stevens and Matt Walker},
  title={Offloading peak processing to virtual farm by STAR experiment at RHIC},
  journal={Journal of Physics: Conference Series},
  volume={368},
  number={1},
  pages={012011},
  url={http://stacks.iop.org/1742-6596/368/i=1/a=012011},
  year={2012},
  abstract={The Virtual Machine framework was used to assemble the STAR-computing environment, validated once, deployed on over 100 8-core VMs at NERSC and Argonne National Lab, and used as a homogeneous Virtual Farm processing events acquired in real time by STAR detector located at Brookhaven National Lab. To provide time dependent calibration, a database snapshot scheme was devised. The two high capacity filesystems, localized at the opposite coasts of US and interconnected via Globus-Online protocol, were used in this setup, which resulted with a highly scalable Cloud-based extension of STAR computing resources. The system was in continuous operation for over 3 months.}
}
@inproceedings{CHEP2015:CloudCMS,
 author = {Colling, D. and others},
 title = {The diverse use of clouds by CMS},
 booktitle = {21st International Conference on Computing in High Energy and Nuclear Physics (CHEP2015)},
 url = {https://indico.cern.ch/event/304944/session/7/contribution/230},
 year = {2015},
}
@inproceedings{CHEP2015:SAMTrain,
 author = {Mengel, M. and Norman, A. and others},
 title = {Replacing the Engines without Stopping The Train; How A Production Data Handling System was Re-engineered and Replaced without anyone Noticing},
 booktitle = {21st International Conference on Computing in High Energy and Nuclear Physics (CHEP2015)},
 url = {https://indico.cern.ch/event/304944/session/4/contribution/463},
 year = {2015},
}
@inproceedings{CHEP2015:ATLASEventService,
 author = {Wenaus, T. and others},
 title = {The ATLAS Event Service: A new approach to event processing},
 booktitle = {21st International Conference on Computing in High Energy and Nuclear Physics (CHEP2015)},
 url = {https://indico.cern.ch/event/304944/session/4/contribution/463},
 year = {2015},
}
@article{1742-6596-513-6-062053,
  author={M Ernst and R Hogue and C Hollowell and W Strecker-Kellog and A Wong and A Zaytsev},
  title={Operating Dedicated Data Centers – Is It Cost-Effective?},
  journal={Journal of Physics: Conference Series},
  volume={513},
  number={6},
  pages={062053},
  url={http://stacks.iop.org/1742-6596/513/i=6/a=062053},
  year={2014},
  abstract={The advent of cloud computing centres such as Amazon's EC2 and Google's Computing Engine has elicited comparisons with dedicated computing clusters. Discussions on appropriate usage of cloud resources (both academic and commercial) and costs have ensued. This presentation discusses a detailed analysis of the costs of operating and maintaining the RACF (RHIC and ATLAS Computing Facility) compute cluster at Brookhaven National Lab and compares them with the cost of cloud computing resources under various usage scenarios. An extrapolation of likely future cost effectiveness of dedicated computing resources is also presented.}
}
@misc{AmazonEC2Pricing,
  title = "{Amazon EC2 Pricing}",
  howpublished = {\url{http://aws.amazon.com/ec2/pricing/}},
}
@inproceedings{CHEP2015:NOvACloud,
 author = {Norman, A.},
 title = {Large Scale Monte Carlo Simulation of neutrino interactions using the Open Science Grid and Commercial Clouds},
 booktitle = {21st International Conference on Computing in High Energy and Nuclear Physics (CHEP2015)},
 url = {https://indico.cern.ch/event/304944/session/9/contribution/465},
 year = {2015},
}
@inproceedings{HEPIX2015:AtlasCloudScale,
 author = {Hover, J.},
 title = {Running ATLAS at scale on Amazon EC2},
 booktitle = {HEPiX Spring 2015 Workshop},
 url = {https://indico.cern.ch/event/346931/session/9/contribution/20},
 year = {2015},
}
@misc{OSGAnnualReport2015,
  title = "{OSG Annual Report 2015}",
  howpublished = {\url{http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=1204}},
}
@article{10.1109/MCSE.2014.80,
author = {John Towns and Timothy Cockerill and Maytal Dahan and Ian Foster and Kelly Gaither and Andrew Grimshaw and Victor Hazlewood and Scott Lathrop and Dave Lifka and Gregory D. Peterson and Ralph Roskies and J. Ray Scott and Nancy Wilkens-Diehr},
title = {XSEDE: Accelerating Scientific Discovery},
journal ={Computing in Science and Engineering},
volume = {16},
number = {5},
issn = {1521-9615},
year = {2014},
pages = {62-74},
doi = {http://doi.ieeecomputersociety.org/10.1109/MCSE.2014.80},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}
@article{1742-6596-396-5-052013,
  author={J Blomer and P Buncic and I Charalampidis and A Harutyunyan and D Larsen and and R Meusel},
  title={Status and future perspectives of CernVM-FS},
  journal={Journal of Physics: Conference Series},
  volume={396},
  number={5},
  pages={052013},
  url={http://stacks.iop.org/1742-6596/396/i=5/a=052013},
  year={2012},
  abstract={The CernVM File System (CernVM-FS) provides a scalable, reliable and close to zero-maintenance software distribution service. It was developed to assist HEP collaborations to deploy their software on the worldwide-distributed computing infrastructure used to run their data processing applications. CernVM-FS is deployed on a wide range of computing resources, ranging from powerful worker nodes at Tier 1 grid sites to simple virtual appliances running on volunteer computers. We present a new approach to stage updates and changes into the file system, which aims to reduce the delay in distributing a software release to less than an hour. In addition, it significantly reduces the complexity with respect to both required capabilities of the master storage as well as installation and maintenance. Furthermore, we will discuss new requirements for additional features on the client side that have arisen from HEP community feedback.}
}